{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on \n",
    "\n",
    "[H. Kervadeca, J. Bouchtibaa, C. Desrosiersa, E. Grangera, J. Dolza, I. Ben Ayed, \"Boundary loss for highly unbalanced segmentation\".](http://proceedings.mlr.press/v102/kervadec19a/kervadec19a.pdf)\n",
    "\n",
    "[D. Karimi and S. E. Salcudean, \"Reducing the Hausdorff Distance in Medical Image Segmentation With Convolutional Neural Networks\".](https://arxiv.org/pdf/1904.10030v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G$ - true mask, $S$ - predicted mask, $\\Delta S = S\\Delta G=(S\\setminus G)\\cup (G\\setminus S)$ - the region of mismatch.\n",
    "\n",
    "$G,\\ S\\subset\\Omega$, $\\Omega$ - the whole channel.\n",
    "\n",
    "\\begin{equation*}\n",
    "dist_k(\\partial G,\\partial S) = \\left(\\int_{\\partial G}\\|y_{\\partial S}(p) - p\\|^k\\,dp\\right)^\\frac{1}{k},\n",
    "\\end{equation*}\n",
    "where $\\|\\cdot\\|$ is Euclidian norm (could choose a different one). $y_{\\partial S}(p)$ - the closest point on $\\partial S$ along the normal vector $n_p$ to $\\partial G$. \n",
    "\n",
    "$D_G(q)=dist(q,\\partial G)= \\inf_{x\\in \\partial G}\\|q-x\\|$ - distance from the point $q$ to $\\partial G$. \n",
    "\n",
    "Parametrizing the segment $[p,y_{\\partial S}(p)]$ \n",
    "\\begin{cases}\n",
    "q = r(\\lambda): = p + \\lambda (y_{\\partial S}(p) - p),\\ \\lambda \\in [0,1],\\\\\n",
    "dq = \\|r'(\\lambda)\\|\\,d\\lambda = \\|y_{\\partial S}(p) - p\\|d\\lambda,\n",
    "\\end{cases}\n",
    "we get\n",
    "\n",
    "\\begin{equation*}\n",
    "\\int_p^{y_{\\partial S}(p)}D^k_G(q) dq = \\int_p^{y_{\\partial S}(p)}\\|q-p\\|^k dq =\n",
    "\\int_0^1\\lambda^k \\|p-y_{\\partial S}(p)\\|^k \\|p-y_{\\partial S}(p)\\| d\\lambda = \\frac{1}{k+1}\\|p-y_{\\partial S}(p)\\|^{k+1}.\n",
    "\\end{equation*}\n",
    "\n",
    "We have\n",
    "\\begin{multline*}\n",
    "(dist_k(\\partial G, \\partial S))^k = \\int_{\\partial G}\\|p - y_{\\partial S}(p)\\|^k\\,dp = k\\int_{\\partial G}\\int_p^{y_{\\partial S}(p)}D^{k-1}_G(q)\\,dq\\,dp =\\\\\n",
    "|iterated\\ integral\\ to\\ double\\ integral|\\leq k\\int_{\\Delta S}D^{k-1}_G(x)\\,dx.\n",
    "\\end{multline*}\n",
    "\n",
    "<b>Rem 1</b> Actually, the above transition from iterated integral to double integral is a neat point. The funny thing is that the difference between these integrals depends on the smoothness of the bdry $G$. They seem to coincide for smooth $G$ but not in general case. The sign of inequality and how different those integrals could be is explained in the picture below\n",
    "\n",
    "[![Iterated vs Double Integral](https://raw.githubusercontent.com/antonsavostianov/bdry_loss/blob/main/iter_vs_double.jpg)](https://github.com/antonsavostianov/bdry_loss/blob/main/iter_vs_double.jpg)\n",
    "\n",
    "The loss that we implement corresponds to the double integral. And the good news is that, as seen from the picture, the double integral is more appropriate for our purposes =) With some abuse of notations, from now on, our $dist_k(\\partial G,\\partial S)$ refers to the double integral, that is \n",
    "\\begin{equation*}\n",
    "(dist_k(\\partial G, \\partial S))^k = k\\int_{\\Delta S}D^{k-1}_G(x)\\,dx.\n",
    "\\end{equation*}\n",
    "\n",
    "<b> Rem 2</b> From just obtained formula it is clear that $dist_1(\\partial G,\\partial S)=|\\Delta S|$.\n",
    "\n",
    "Let us rewrite the above formula via integral over the whole region $\\Omega$\n",
    "\\begin{multline*}\n",
    "(dist_k(\\partial G, \\partial S))^k = k\\left(\\int_{S\\setminus G}D^{k-1}_G(x)\\,dx+\\int_{G\\setminus S}D^{k-1}_G(x)\\,dx\\right)=\\\\ \n",
    "k\\left(\\int_{S\\setminus G}D^{k-1}_G(x)\\,dx-\\int_{S\\cap G}D^{k-1}_G(x)\\,dx\\right)+k\\left(\\int_{S\\cap G}D^{k-1}_G(x)\\,dx+\\int_{G\\setminus S}D^{k-1}_G(x)\\,dx\\right)=\\\\\n",
    "k\\left(\\int_{S\\setminus G}D^{k-1}_G(x)\\,dx-\\int_{S\\cap G}D^{k-1}_G(x)\\,dx\\right)-k\\left(-\\int_{S\\cap G}D^{k-1}_G(x)\\,dx-\\int_{G\\setminus S}D^{k-1}_G(x)\\,dx\\right).\n",
    "\\end{multline*}\n",
    "\n",
    "Introducing the function $\\phi_G(x)$ (can be precomputed)\n",
    "\\begin{equation*}\n",
    "\\phi_G(x) =\n",
    "\\begin{cases}\n",
    "kD^{k-1}_G(x),\\ x\\notin G,\\\\\n",
    "-kD^{k-1}_G(x),\\ x\\in G,\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "we find\n",
    "\\begin{multline*}\n",
    "(dist_k(\\partial G, \\partial S))^k = \\int_S\\phi_G(x)\\,dx - \\int_G\\phi_G(x)\\,dx =\\\\\n",
    "\\int_\\Omega \\phi_G(x)1_S(x)\\,dx - \\int_\\Omega \\phi_G(x)1_G(x)\\,dx= \\int_\\Omega\\phi_G(x)(1_S(x)-1_G(x))\\,dx, \n",
    "\\end{multline*}\n",
    "that is \n",
    "\\begin{equation*}\n",
    "(dist_k(\\partial G, \\partial S))^k = \\int_\\Omega\\phi_G(x)(1_S(x)-1_G(x))\\,dx. \n",
    "\\end{equation*}\n",
    "\n",
    "<b>Rem 3</b> We also have\n",
    "\\begin{equation*}\n",
    "(dist_k(\\partial G, \\partial S))^k \\leq k\\int_{\\Omega}D^{k-1}_G(x)|1_S(x)-1_G(x)|^\\alpha\\,dx,\\ \\mbox{for all }\\alpha\\geq 0.\n",
    "\\end{equation*}\n",
    "Indeed, for any $\\alpha\\geq 0$ we have\n",
    "\\begin{multline*}\n",
    "(dist_k(\\partial G, \\partial S))^k = k\\left(\\int_{S\\setminus G}D^{k-1}_G(x)\\,dx+\\int_{G\\setminus S}D^{k-1}_G(x)\\,dx\\right)=\\\\\n",
    "k\\left(\\int_{S\\setminus G}D^{k-1}_G(x)(1_S(x)-1_G(x))\\,dx+\\int_{G\\setminus S}D^{k-1}_G(x)(1_G(x)-1_S(x))\\,dx\\right)=\\\\\n",
    "k\\int_{S\\Delta G}D^{k-1}_G(x)|1_S(x)-1_G(x)|^\\alpha\\,dx\\leq k\\int_{\\Omega}D^{k-1}_G(x)|1_S(x)-1_G(x)|^\\alpha\\,dx.\n",
    "\\end{multline*}\n",
    "\n",
    "<b>Rem 4</b> We can get rid of $\\phi_G(x)$ in $dist_k(\\partial G, \\partial S)$. This also shows a connection to some other losses considered in the literature.\n",
    "\\begin{multline*}\n",
    "(dist_k(\\partial G, \\partial S))^k = \\int_\\Omega\\phi_G(x)(1_S(x)-1_G(x))\\,dx = \\\\\n",
    "\\int_{\\Omega\\setminus G}\\phi_G(x)(1_S(x)-1_G(x))\\,dx + \\int_G\\phi_G(x)(1_S(x)-1_G(x))\\,dx=\\\\\n",
    "k\\int_{\\Omega\\setminus G}D^{k-1}_G(x)1_S(x)\\,dx +k\\int_G D^{k-1}_G(x)(1_G(x)-1_S(x))\\,dx =\\\\\n",
    "\\Big|1_S(x)= |1_G(x)-1_S(x)|,\\ x\\in\\Omega\\setminus G\\Big|=\n",
    "k\\int_\\Omega D_G^{k-1}(x)|1_G(x)-1_S(x)|dx, \n",
    "\\end{multline*}\n",
    "that is\n",
    "\\begin{equation*}\n",
    "(dist_k(\\partial G, \\partial S))^k = k\\int_\\Omega D_G^{k-1}(x)|1_G(x)-1_S(x)|dx. \n",
    "\\end{equation*}\n",
    "\n",
    "In the case the predicted segmentation mask $S$ is given in the form of probability density $p_{S}(x)$ the natural generalization of the above boundary loss is\n",
    "\\begin{equation*}\n",
    "Loss_k(\\theta) = (dist_k(\\partial G, \\partial S_\\theta))^k =  k\\int_{\\Omega}D^{k-1}_G(x)|1_G(x)-p_{S_\\theta}(x)|\\,dx.\n",
    "\\end{equation*}\n",
    "We implement the loss in just computed form. \n",
    "\n",
    "<b>Rem 5</b> Now the form for two-sided Hausdorff distance and its possible variants is clear \n",
    "\n",
    "\\begin{equation*}\n",
    "SymLoss_k(\\theta) = (dist^{sym}_k(\\partial G, \\partial S_\\theta))^k = max \\Big\\{k\\int_{\\Omega}D^{k-1}_G(x)|1_G(x)-p_{S_\\theta}(x)|\\,dx, k\\int_{\\Omega}D^{k-1}_{S_\\theta}(x)|1_G(x)-p_{S_\\theta}(x)|\\,dx\\Big\\},\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "SumLoss_k(\\theta) = (dist_k(\\partial G, \\partial S_\\theta))^k+ (dist_k(\\partial S_\\theta,\\partial G))^k=\n",
    "k\\int_{\\Omega}D^{k-1}_G(x)|1_G(x)-p_{S_\\theta}(x)|\\,dx + k\\int_{\\Omega}D^{k-1}_{S_\\theta}(x)|1_G(x)-p_{S_\\theta}(x)|\\,dx.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from scipy.ndimage import distance_transform_edt as distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function $kD_G^{k-1}(x)$, can be precomputed for true mask\n",
    "def dist_2segbdry(seg: Tensor, k=2)->Tensor:\n",
    "    \"\"\"\n",
    "        seg - Tensor of shape \n",
    "                (b,c,h,w) if dim = 2\n",
    "                (b,c,h,w,d) if dim = 3.\n",
    "              consists of 0s and 1s, 1s correspond to the segmentation mask  \n",
    "        \n",
    "        this function may accept a method or argument that determines how to \n",
    "        calculate the distance transform. In general it is not necessarily\n",
    "        the euclidian distance transform from scipy, which is implemented now.\n",
    "    \"\"\"\n",
    "    \n",
    "    B, C = seg.shape[:2]\n",
    "    \n",
    "    res = torch.zeros_like(seg)\n",
    "    \n",
    "    # mask for the actual segmentaion\n",
    "    posmask = seg.bool()\n",
    "    # mask for the complement to segmentation mask\n",
    "    negmask = ~posmask\n",
    "    \n",
    "    # there is no option \"along axis\" for distance_transform_edt so we iterate \n",
    "    for b in range(B):\n",
    "        for c in range(C):\n",
    "            #            dist^{k-1}(x, bdry seg), x outside seg              dist^{k-1}(x, bdry seg), x inside seg     \n",
    "            res[b,c] = k*( negmask[b,c]*(distance(negmask[b,c])**(k-1)) + posmask[b,c]*(distance(posmask[b,c])**(k-1)) )\n",
    "            # here distance - computes distances from 1s to 0s - see the check below \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dist_2SegBdry(nn.Module):\n",
    "    \"\"\"\n",
    "        calculates the distance to the segmentation mask bdry\n",
    "        see the detailed description for the fn dist_2segbdry\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Dist_2SegBdry, self).__init__()\n",
    "    def forward(seg: Tensor, k=2)->Tensor:\n",
    "        return dist_2segbdry(seg, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bdry_loss(other_seg: Tensor, seg: Tensor, dt_seg: Tensor = None, k=2):\n",
    "    \"\"\"\n",
    "        all input tesors have the same shape: \n",
    "            (b,c,h,w) if dim = 2,\n",
    "            (b,c,h,w,d) if dim = 3.\n",
    "            \n",
    "        seg - true_mask, consists of 0s and 1s,\n",
    "        \n",
    "        other_seg - predicted segmentation\n",
    "                    can be soft (tensor of probabilities) or hard (tensor of 0s and 1s)\n",
    "        \n",
    "        dt_seg - precomputed dist_2segbdry_2d(seg,k), preferred to seg\n",
    "        dt - for distance transform, in general it is not neccessarily\n",
    "                  given by distance_transform_edt from scipy but can be a different one,\n",
    "                  for example based on colnvolutions, erosions or using a different metric\n",
    "        \n",
    "        each channel c corresponds to one class\n",
    "        1s corresponds to the segmentation mask, 0s - to the complement\n",
    "        k - distance degree\n",
    "    \"\"\"\n",
    "    if dt_seg is not None:\n",
    "        assert dt_seg.shape == other_seg.shape\n",
    "        return (dt_seg*torch.abs(other_seg-seg)).mean()\n",
    "    else:\n",
    "        assert seg.shape == other_seg.shape\n",
    "        return (dist_2segbdry(seg, k)*torch.abs(other_seg-seg)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BdryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        Calculates the bdry loss\n",
    "        See the detailed description for the fn bdry_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super(BdryLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "    def forward(other_seg: Tensor, seg: Tensor,  dt_seg: Tensor = None, k=2):\n",
    "        return bdry_loss(other_seg = other_seg, seg = seg, dt_seg = dt_seg, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Boundary Loss: via max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_bdry_loss(other_seg: Tensor, seg: Tensor, dt_seg: Tensor = None,  k=2):\n",
    "    \"\"\"\n",
    "        all input tesors have the same shape: \n",
    "            (b,c,h,w) if dim = 2,\n",
    "            (b,c,h,w,d) if dim = 3.\n",
    "        \n",
    "        seg - true_mask, consists of 0s and 1s;\n",
    "        \n",
    "        \n",
    "        other_seg - predicted segmentation,\n",
    "                    can be soft (tensor of probabilities) or hard (tensor of 0s and 1s)\n",
    "                    \n",
    "        dt_seg - precomputed dist_2segbdry_2d(seg,k) \n",
    "        \n",
    "        each channel c corresponds to one class\n",
    "        1s corresponds to the segmentation mask, 0 - to the complement\n",
    "        k - distance degree\n",
    "    \"\"\"\n",
    "    \n",
    "    assert dt_seg.shape == other_seg.shape and seg.shape == other_seg.shape\n",
    "    \n",
    "    # other_seg_01 - the tensor of probabilities mapped to 01-tensor\n",
    "    # may be useful to make thresholds for each channel\n",
    "    other_seg_01 = (other_seg>0.5).bool()\n",
    "    \n",
    "    return torch.max(\n",
    "        Tensor([bdry_loss(other_seg = other_seg, seg = seg, dt_seg = dt_seg),\n",
    "                bdry_loss(seg = other_seg_01, other_seg = seg)])\n",
    "    ) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymBdryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        Calculates symmetric boundary loss via max\n",
    "        See the detailed description for the fn sym_bdry_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super(SymBdryLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "    def forward(other_seg: Tensor, seg: Tensor, dt_seg: Tensor = None, k=2):\n",
    "        return sym_bdry_loss(other_seg = other_seg, seg = seg, dt_seg = dt_seg, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Boundary Loss: via sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_bdry_loss(other_seg: Tensor, seg: Tensor, dt_seg: Tensor = None,  k=2):\n",
    "    \"\"\"\n",
    "        all input tesors have the same shape: \n",
    "            (b,c,h,w) if dim = 2,\n",
    "            (b,c,h,w,d) if dim = 3.\n",
    "        \n",
    "        seg - true_mask, consists of 0s and 1s;\n",
    "        \n",
    "        other_seg - predicted segmentation,\n",
    "                    can be soft (tensor of probabilities) or hard (tensor of 0s and 1s);\n",
    "                    \n",
    "        dt_seg - precomputed dist_2segbdry_2d(seg,k); \n",
    "        \n",
    "        each channel c corresponds to one class\n",
    "        1s corresponds to the segmentation mask, 0 - to the complement\n",
    "        k - distance degree\n",
    "    \"\"\"\n",
    "    assert dt_seg.shape == other_seg.shape and seg.shape == other_seg.shape\n",
    "    \n",
    "    # other_seg_01 - the tensor of probabilities mapped to 01-tensor\n",
    "    # may be useful to make thresholds for each channel\n",
    "    other_seg_01 = (other_seg>0.5).bool()\n",
    "    \n",
    "    return bdry_loss(other_seg = other_seg, seg = seg, dt_seg = dt_seg) +\n",
    "            bdry_loss(seg = other_seg_01, other_seg = seg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumBdryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        Calculate sum_bdry_loss\n",
    "        See the detailed description for the fn sum_bdry_loss\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super(SumBdryLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "    def forward(other_seg: Tensor, seg: Tensor, dt_seg: Tensor = None, k=2):\n",
    "        return sum_bdry_loss(other_seg = other_seg, seg = seg, dt_seg = dt_seg, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
